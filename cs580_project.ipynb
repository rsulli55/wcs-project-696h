{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "termdf = pd.read_csv('term.txt', sep='\\t', header=None, names=[\n",
    "                     'language', 'speaker', 'chip', 'term_abbrev'])\n",
    "dictdf = pd.read_csv('dict.txt', sep='\\t', skiprows=[0], names=[\n",
    "                     'language', 'term', 'translation', 'term_abbrev'])\n",
    "chipdf = pd.read_csv('chip.txt', sep='\\t', names=[\n",
    "                     'chip', 'letter', 'number', 'letternumber'])\n",
    "\n",
    "# mappings between different indices\n",
    "num_to_chip = {}\n",
    "for c in range(1, 331):\n",
    "    letter = chipdf.loc[chipdf['chip'] == c].iloc[0]['letter']\n",
    "    number = chipdf.loc[chipdf['chip'] == c].iloc[0]['number']\n",
    "    num_to_chip[c] = (letter, number)\n",
    "\n",
    "chip_to_num = {}\n",
    "for n in num_to_chip:\n",
    "    chip_to_num[num_to_chip[n]] = n\n",
    "\n",
    "table_to_chipnum = {}\n",
    "for i, c in enumerate('ABCDEFGHIJ'):\n",
    "    for j in range(41):\n",
    "        if (c == 'A' or c == 'J') and j > 0:\n",
    "            continue\n",
    "        table_to_chipnum[(i,j)] = chip_to_num[(c, j)]\n",
    "\n",
    "languages = [16, 20, 51, 56, 60, 64, 74, 87]\n",
    "bct_counts = {16: 6, 20: 7, 51: 5, 56: 4, 60: 4, 64: 5, 74: 4, 87: 5}\n",
    "num_chips = 330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "seed = 8\n",
    "# to test other seeds uncomment next line\n",
    "# seed = rng.integers(0, 10)\n",
    "\n",
    "# heatmap plotting function following the guide in the matplotlib documentation\n",
    "def plot_heatmap(data, ax):\n",
    "    im = ax.imshow(data, cmap = 'hot_r')\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "\n",
    "    ax.set_xticklabels([str(i) for i in range(41)])\n",
    "    ax.set_yticklabels([c for c in 'ABCDEFGHIJ'])\n",
    "\n",
    "    ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "\n",
    "    for _, spine in ax.spines.items():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    return im\n",
    "\n",
    "# build the document word matrix for each language in languages\n",
    "def build_word_counts(languages):\n",
    "    word_counts = {}\n",
    "    for language in languages:\n",
    "        lang_dict = dictdf[dictdf['language'] == language]\n",
    "        lang_terms = termdf[termdf['language'] == language]\n",
    "        num_terms = lang_dict['term'].max()\n",
    "        abbreviations = lang_dict['term_abbrev'].unique()\n",
    "        termabbrev_map = {}\n",
    "        for abbrev in abbreviations:\n",
    "            # use the smallest term index for the abbreviation\n",
    "            subset = lang_dict['term_abbrev'] == abbrev\n",
    "            termabbrev_map[abbrev] = lang_dict[subset]['term'].min()\n",
    "\n",
    "        word_count = np.zeros((num_terms, num_chips), dtype=int)\n",
    "        for abbrev in abbreviations:\n",
    "            for chip in range(num_chips):\n",
    "                subset = (lang_terms['term_abbrev'] == abbrev) & (lang_terms['chip'] == chip + 1)\n",
    "                word_count[termabbrev_map[abbrev]-1, chip] = lang_terms[subset]['chip'].count()\n",
    "        \n",
    "        word_counts[language] = word_count.copy()\n",
    "        \n",
    "    return word_counts\n",
    "\n",
    "# build a topics over chips model   \n",
    "def topics_over_chips(word_count, num_categories):\n",
    "    lda = LatentDirichletAllocation(n_components=num_categories, random_state = seed, max_doc_update_iter = 500, max_iter= 100)\n",
    "    lda.fit(word_count)\n",
    "    return lda.components_\n",
    "\n",
    "# build a topics over terms model\n",
    "def topics_over_terms(word_count, num_categories):\n",
    "    lda = LatentDirichletAllocation(n_components=num_categories, random_state = seed, max_doc_update_iter = 500, max_iter= 100)\n",
    "    lda.fit(word_count.T)\n",
    "    return lda.components_\n",
    "\n",
    "# plot histograms for topics in the topics over terms model\n",
    "def topics_over_terms_histograms(language, topics):\n",
    "    # normalization for histograms\n",
    "    topics /= topics.sum(axis=1)[:, np.newaxis]\n",
    "    num_categories = topics.shape[0]\n",
    "    num_rows = num_categories\n",
    "    num_cols = 1\n",
    "    if num_categories > 5:\n",
    "        num_cols = 2\n",
    "        num_rows = 5\n",
    "    \n",
    "    fig, axs = plt.subplots(num_rows, num_cols, sharey=True, figsize=(8, 12))\n",
    "    for tid, topic in enumerate(topics):\n",
    "        maximum = np.max(topic)\n",
    "        minimum = np.max(topic)\n",
    "        # we say something is representative if it has probability > 0.6\n",
    "        # and potentially representative if it has probability > 0.3\n",
    "        rep_threshold =  0.6\n",
    "        pot_threshold =  0.3\n",
    "        representative = topic >= rep_threshold\n",
    "        pot_representative = (topic >= pot_threshold) & (topic < rep_threshold)\n",
    "        not_representative = topic < pot_threshold\n",
    "        bars = [topic[representative], topic[pot_representative], topic[not_representative]]\n",
    "        \n",
    "        if num_categories > 5:\n",
    "            ax = axs[tid % num_rows, tid // num_rows]\n",
    "        else:\n",
    "            ax = axs[tid]\n",
    "        bin_edges = ax.hist(bars, color = ['g', 'b', 'r'], histtype='barstacked', \n",
    "                     edgecolor = 'black', label = ['Representative', 'Potentially', 'Nonrepresentative'])\n",
    "        if tid < num_rows:\n",
    "            plt.setp(ax, ylabel='Count')\n",
    "        \n",
    "        if maximum < 0.1:\n",
    "            ax.xaxis.set_major_formatter(mticker.FormatStrFormatter('%.1e'))\n",
    "\n",
    "        if tid == 0:\n",
    "            ax.legend()\n",
    "    \n",
    "    if num_categories > 5:\n",
    "        for ind in range(num_categories, 10):\n",
    "            ax = axs[ind % num_rows, ind // num_rows]\n",
    "            ax.remove()\n",
    "            ax  = None \n",
    "    plt.savefig('lang{}_{}topichistSEED{}.png'.format(language, num_categories, seed))\n",
    "\n",
    "# create an topics over terms LDA model for each number in `num_cats_list`\n",
    "# and plot topics over terms histograms for each\n",
    "def test_num_topics(language, num_cats_list):\n",
    "    word_count = word_counts[language]\n",
    "    for num in num_cats_list:\n",
    "        lda = LatentDirichletAllocation(n_components=num, random_state = seed, max_doc_update_iter = 500, max_iter= 100)\n",
    "        lda.fit(word_count.T)\n",
    "        topics = lda.components_\n",
    "        \n",
    "        topics_over_terms_histograms(language, topics)\n",
    "        rep_terms = find_representative_terms(language, topics)\n",
    "        print('For num = ', num, 'representative terms:')\n",
    "        print(rep_terms)\n",
    "\n",
    "# find the represetative terms of the topics using\n",
    "# topics over terms model\n",
    "def find_representative_terms(language, topics):\n",
    "    rep_terms = {}\n",
    "    for tid, topic in enumerate(topics):\n",
    "        maximum = np.max(topic)\n",
    "        minimum = np.max(topic)\n",
    "        # we say something is representative if it has probability > 0.6\n",
    "        rep_threshold =  0.6\n",
    "        representatives = np.nonzero(topic >= rep_threshold)[0]\n",
    "        lang_dict = dictdf[dictdf['language'] == language]\n",
    "           \n",
    "        abbrevs = {}\n",
    "        for rep in representatives:\n",
    "            abbrev = lang_dict.loc[lang_dict['term'] == rep + 1]['term_abbrev'].iloc[0] \n",
    "            abbrevs[rep+1] = abbrev\n",
    "        \n",
    "        rep_terms[tid] = abbrevs\n",
    "    return rep_terms\n",
    "    \n",
    "# plot heatmaps for the topic distributions using\n",
    "# the topics over chips model\n",
    "def topics_over_chips_heatmaps(language, topics):\n",
    "    # normalization for colormaps\n",
    "    topics_max = np.max(topics)\n",
    "    num_categories = topics.shape[0]\n",
    "    num_rows = num_categories\n",
    "    num_cols = 1\n",
    "    if num_categories > 4:\n",
    "        num_cols = 2\n",
    "        num_rows = 4\n",
    "    \n",
    "    fig, ax = plt.subplots(num_categories, 1)\n",
    "    for tid, topic in enumerate(topics):\n",
    "        data = np.zeros((10, 41))\n",
    "        data[0, 40] = topics_max\n",
    "        for i in range(data.shape[0]):\n",
    "            for j in range(data.shape[1]):\n",
    "                if (i, j) in table_to_chipnum:\n",
    "                    data[i, j] = topic[table_to_chipnum[(i, j)] - 1]\n",
    "        \n",
    "        im = plot_heatmap(data, ax[tid])\n",
    "        ax[tid].set_title('Topic {}'.format(tid))\n",
    "    \n",
    "    fig.set_size_inches((20, 5*num_categories))\n",
    "    plt.savefig('lang{}_topicmapSEED{}.png'.format(language, seed))\n",
    "\n",
    "# finds the most strongly associated topic for each color term in bcts\n",
    "def topics_over_chips_best_topics(language, num_categories, bcts):\n",
    "    word_count = word_counts[language]\n",
    "    lda = LatentDirichletAllocation(n_components=num_categories, random_state = seed, max_doc_update_iter = 500, max_iter= 100)\n",
    "    lda.fit(word_count)\n",
    "    term_dists = lda.transform(word_count)\n",
    "    best_topics = {}\n",
    "    \n",
    "    for bct in bcts:\n",
    "        dist = term_dists[bct-1, :]\n",
    "        tid = np.argmax(dist)\n",
    "        best_topics[bct] = (tid, dist[tid])\n",
    "        \n",
    "    return best_topics\n",
    "   \n",
    "# get the data used for heatmaps on the topics over chips model\n",
    "def topics_over_chips_data(language, topics):\n",
    "    # normalization for colormaps\n",
    "    topics_max = np.max(topics)\n",
    "    topics /= topics_max\n",
    "    num_categories = topics.shape[0]\n",
    "    \n",
    "    to_return = {}\n",
    "    \n",
    "    fig, ax = plt.subplots(num_categories, 1)\n",
    "    for tid, topic in enumerate(topics):\n",
    "        data = np.zeros((10, 41))\n",
    "        data[0, 40] = topics_max\n",
    "        for i in range(data.shape[0]):\n",
    "            for j in range(data.shape[1]):\n",
    "                if (i, j) in table_to_chipnum:\n",
    "                    data[i, j] = topic[table_to_chipnum[(i, j)] - 1]\n",
    "        \n",
    "        to_return[tid] = data\n",
    "    \n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the document word matrices\n",
    "word_counts = build_word_counts(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train an LDA models for each language with the number of topics\n",
    "# equal to the BCT counts given in literature\n",
    "chip_topics = {}\n",
    "term_topics = {}\n",
    "for language in languages:\n",
    "    chip_topics[language] = topics_over_chips(word_counts[language], bct_counts[language])\n",
    "    term_topics[language] = topics_over_terms(word_counts[language], bct_counts[language])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for languages 20 and 16, try to determine the number of color categories\n",
    "# by training models with various number of topics\n",
    "print('Language 20')\n",
    "test_num_topics(language = 20, num_cats_list=[4, 5, 6, 7, 8, 9, 10])\n",
    "print('\\nLanguage 16')\n",
    "test_num_topics(language = 16, num_cats_list=[2, 3, 4, 5, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each language plot the topic distribution histograms\n",
    "# and heatmaps for the topics over chips model\n",
    "for language in languages:\n",
    "    topics_over_terms_histograms(language, term_topics[language])\n",
    "    topics_over_chips_heatmaps(language, chip_topics[language])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the cosine similarity of two matrices\n",
    "# by flattening them into arrays\n",
    "def cosine_sim(X, Y):\n",
    "    vecX = X.flatten()\n",
    "    vecY = Y.flatten()\n",
    "    \n",
    "    return np.dot(vecX, vecY) / (np.linalg.norm(vecX) * np.linalg.norm(vecY))\n",
    "\n",
    "# how similar are the color categories the model finds with the color categories\n",
    "# from the literature?\n",
    "lang16bcts = range(1, 7)\n",
    "lang16_bct_data = {}\n",
    "lang20bcts = [1, 2, 4, 5, 6, 12, 13]\n",
    "lang20_bct_data = {}\n",
    "\n",
    "for i in lang16bcts: \n",
    "    df = pd.read_csv('Data for Ryan/L16_W{}.csv'.format(i), header = None)\n",
    "    lang16_bct_data[i] = df.to_numpy()\n",
    "\n",
    "for i in lang20bcts: \n",
    "    df = pd.read_csv('Data for Ryan/L20_W{}.csv'.format(i), header = None)\n",
    "    lang20_bct_data[i] = df.to_numpy()\n",
    "\n",
    "lang16heatmaps = topics_over_chips_data(16, chip_topics[16])    \n",
    "lang20heatmaps = topics_over_chips_data(20, chip_topics[20])    \n",
    "\n",
    "lang16_matches = {}\n",
    "lang20_matches = {}\n",
    "print('Lang 16 Comparison\\n')\n",
    "for tid in lang16heatmaps:\n",
    "    print('Topic {}'.format(tid))\n",
    "    best_sim = 0\n",
    "    for i in lang16bcts:\n",
    "        frobenius = np.linalg.norm(lang16heatmaps[tid] - lang16_bct_data[i])\n",
    "        l1 = np.linalg.norm(lang16heatmaps[tid] - lang16_bct_data[i], ord = 1)\n",
    "        cos_sim = cosine_sim(lang16heatmaps[tid], lang16_bct_data[i])\n",
    "        if cos_sim > best_sim:\n",
    "            lang16_matches[tid] = i\n",
    "            best_sim = cos_sim\n",
    "        print('BCT: {}, frob: {}, L1: {}, cos : {}'.format(i, frobenius, l1, cos_sim))\n",
    "    print()\n",
    "\n",
    "print('Lang 20 Comparison\\n')\n",
    "for tid in lang20heatmaps:\n",
    "    print('Topic {}'.format(tid))\n",
    "    best_sim = 0\n",
    "    for i in lang20bcts:\n",
    "        frobenius = np.linalg.norm(lang20heatmaps[tid] - lang20_bct_data[i])\n",
    "        l1 = np.linalg.norm(lang20heatmaps[tid] - lang20_bct_data[i], ord = 1)\n",
    "        cos_sim = cosine_sim(lang20heatmaps[tid], lang20_bct_data[i])\n",
    "        if cos_sim > best_sim:\n",
    "            lang20_matches[tid] = i\n",
    "            best_sim = cos_sim\n",
    "        print('BCT: {}, frob: {}, L1: {}, cos : {}'.format(i, frobenius, l1, cos_sim))\n",
    "    print()\n",
    "    \n",
    "print('Lang 16 Matches:', lang16_matches)\n",
    "print('Lang 20 Matches:', lang20_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the best topics using the topics over chips model\n",
    "print('Lang 20 best_topics:\\n')\n",
    "lang20_best_topics = topics_over_chips_best_topics(20, 7, lang20bcts)\n",
    "print(lang20_best_topics)\n",
    "print('\\nLang 16 best_topics:\\n')\n",
    "lang16_best_topics = topics_over_chips_best_topics(16, 7, lang16bcts)\n",
    "print(lang16_best_topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
